{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67181cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Big File that runs all\n",
    "'''\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning import _logger as log\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateLogger\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbc7caf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1997"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(1997)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e35d3ae",
   "metadata": {},
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--backbone',\n",
    "                    default='efficientnet-b0',\n",
    "                    type=str,\n",
    "                    metavar='BK',\n",
    "                    help='Name as in segmentation_models_pytorch')\n",
    "parser.add_argument('--data-path',\n",
    "                    default='/Users/home/fun/comma10k-exp/data/',\n",
    "                    type=str,\n",
    "                    metavar='DP',\n",
    "                    help='data_path')\n",
    "parser.add_argument('--epochs',\n",
    "                    default=30,\n",
    "                    type=int,\n",
    "                    metavar='N',\n",
    "                    help='total number of epochs')\n",
    "parser.add_argument('--batch-size',\n",
    "                    default=32,\n",
    "                    type=int,\n",
    "                    metavar='B',\n",
    "                    help='batch size',\n",
    "                    dest='batch_size')\n",
    "parser.add_argument('--gpus',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help='number of gpus to use')\n",
    "parser.add_argument('--lr',\n",
    "                    '--learning-rate',\n",
    "                    default=1e-4,\n",
    "                    type=float,\n",
    "                    metavar='LR',\n",
    "                    help='initial learning rate',\n",
    "                    dest='LR')\n",
    "parser.add_argument('--eps',\n",
    "                    default=1e-7,\n",
    "                    type=float,\n",
    "                    help='eps for adaptive optimizers',\n",
    "                    dest='eps')\n",
    "parser.add_argument('--height',\n",
    "                    default=14*32,\n",
    "                    type=int,\n",
    "                    help='image height')\n",
    "parser.add_argument('--width',\n",
    "                    default=18*32,\n",
    "                    type=int,\n",
    "                    help='image width')\n",
    "parser.add_argument('--num-workers',\n",
    "                    default=6,\n",
    "                    type=int,\n",
    "                    metavar='W',\n",
    "                    help='number of CPU workers',\n",
    "                    dest='num_workers')\n",
    "parser.add_argument('--weight-decay',\n",
    "                    default=1e-3,\n",
    "                    type=float,\n",
    "                    metavar='WD',\n",
    "                    help='Optimizer weight decay')\n",
    "parser.add_argument('--version',\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    metavar='V',\n",
    "                    help='version or id of the net')\n",
    "parser.add_argument('--resume-from-checkpoint',\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    metavar='RFC',\n",
    "                    help='path to checkpoint')\n",
    "parser.add_argument('--seed-from-checkpoint',\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    metavar='SFC',\n",
    "                    help='path to checkpoint seed')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9da694e",
   "metadata": {},
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264a8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf8f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def pad_to_multiple(x, k=32):\n",
    "    return int(k*(np.ceil(x/k)))\n",
    "\n",
    "def get_train_transforms(height = 437, \n",
    "                         width = 582): \n",
    "    return A.Compose([\n",
    "            A.Resize(height=height, width=width, p=1.0),\n",
    "            A.PadIfNeeded(pad_to_multiple(height), \n",
    "                          pad_to_multiple(width), \n",
    "                          border_mode=cv2.BORDER_CONSTANT, \n",
    "                          value=0, \n",
    "                          mask_value=0)\n",
    "        ], p=1.0)\n",
    "\n",
    "def get_valid_transforms(height: int = 437, \n",
    "                         width: int = 582): \n",
    "    return A.Compose([\n",
    "            A.Resize(height=height, width=width, p=1.0),\n",
    "            A.PadIfNeeded(pad_to_multiple(height), \n",
    "                          pad_to_multiple(width), \n",
    "                          border_mode=cv2.BORDER_CONSTANT, \n",
    "                          value=0, \n",
    "                          mask_value=0)\n",
    "        ], p=1.0)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "        A.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return A.Compose(_transform)\n",
    "\n",
    "\n",
    "class CommaLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, images_path, preprocess_fn, transforms, class_values):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.images_path = images_path\n",
    "        self.transforms = transforms\n",
    "        self.preprocess = get_preprocessing(preprocess_fn)\n",
    "        self.class_values = class_values\n",
    "        self.images_folder = 'imgs'\n",
    "        self.masks_folder = 'masks'\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images_path[idx]\n",
    "        img = cv2.imread(str(self.data_path/self.images_folder/image))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(str(self.data_path/self.masks_folder/image), 0).astype('uint8')\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=img, mask=mask)\n",
    "            img = sample['image']\n",
    "            mask = sample['mask']\n",
    "        \n",
    "        mask = np.stack([(mask == v) for v in self.class_values], axis=-1).astype('uint8')\n",
    "        \n",
    "        if self.preprocess:\n",
    "            sample = self.preprocess(image=img, mask=mask)\n",
    "            img = sample['image']\n",
    "            mask = sample['mask']\n",
    "            \n",
    "        return img, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1da97a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n| Model Class |\\n- __init__         :: set up params for Trainer\\n-  model           :: build model Unet \\n-  foward          :: forward pass; return logits\\n-  loss            :: loss function\\n-  training step   :: Forward pass -> loss\\n-  validation_step :: Forward pass -> loss\\n-  configure optim :: configure optimizers\\n-  check data      :: check whether or not we have the files\\n-  setup           :: setup dataset for the dataloader\\n-  dataloaders      :: train / validation loaders\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "| Model Class |\n",
    "- __init__         :: set up params for Trainer\n",
    "-  model           :: build model Unet \n",
    "-  foward          :: forward pass; return logits\n",
    "-  loss            :: loss function\n",
    "-  training step   :: Forward pass -> loss\n",
    "-  validation_step :: Forward pass -> loss\n",
    "-  configure optim :: configure optimizers\n",
    "-  check data      :: check whether or not we have the files\n",
    "-  setup           :: setup dataset for the dataloader\n",
    "-  dataloaders      :: train / validation loaders\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd78b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(pl.LightningModule):\n",
    "    # Transfer Learning\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_path='/Users/home/fun/comma10k-exp/data/',\n",
    "                 backbone='efficientnet-b0',\n",
    "                 batch_size=8,\n",
    "                 lr=1e-4,\n",
    "                 eps=1e-7,\n",
    "                 height=14*32,\n",
    "                 width=18*32,\n",
    "                 #num_workers=1,\n",
    "                 epochs=30,\n",
    "                 gpus=0,\n",
    "                 weight_decay=1e-3,\n",
    "                 class_values=[41, 76, 90, 124, 161, 0], **kwargs):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.data_path = Path(data_path)\n",
    "        self.epochs = epochs\n",
    "        self.backbone = backbone\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        # self.num_workers = num_workers\n",
    "        self.gpus = gpus\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.class_values = class_values\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.preprocess_fn = smp.encoders.get_preprocessing_fn(self.backbone, pretrained='imagenet')\n",
    "        \n",
    "        self.__build_model()\n",
    "        \n",
    "    \n",
    "    def __build_model(self):\n",
    "        # Define model layers & loss \n",
    "        \n",
    "        self.net = smp.Unet(self.backbone, classes=len(self.class_values),\n",
    "                            activation=None, encoder_weights='imagenet')\n",
    "        \n",
    "        self.loss_func = lambda x, y: torch.nn.CrossEntropyLoss()(x, torch.argmax(y, axis=1))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass. Returns logits\n",
    "        return self.net(x)\n",
    "        \n",
    "    def loss(self, logits, labels):\n",
    "        # use the loss_function\n",
    "        return self.loss_func(logits, labels)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # Forward Pass \n",
    "        x, y = batch\n",
    "        y_logits = self.forward(x)\n",
    "        \n",
    "        # compute loss and accuracy\n",
    "        train_loss = self.loss(y_logits, y)\n",
    "        \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        # self.log(\"train_loss\", train_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # Forward pass\n",
    "        x, y = batch\n",
    "        y_logits = self.forward(x)\n",
    "        \n",
    "        # Compute loss and accuracy\n",
    "        val_loss = self.loss(y_logits, y)\n",
    "        \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        # self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.Adam\n",
    "        optimizer_kwargs = {'eps': self.eps}\n",
    "        \n",
    "        optimizer = optimizer(self.parameters(),\n",
    "                              lr=self.lr,\n",
    "                              weight_decay=self.weight_decay,\n",
    "                              **optimizer_kwargs)\n",
    "\n",
    "        scheduler_kwargs = {'T_max': self.epochs*len(self.train_dataset)//self.batch_size,\n",
    "                            'eta_min':self.lr/50} \n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "        interval = 'step'\n",
    "        scheduler = scheduler(optimizer, **scheduler_kwargs)\n",
    "\n",
    "        return [optimizer], [{'scheduler':scheduler, 'interval': interval, 'name': 'lr'}]\n",
    "    \n",
    "    \n",
    "    def check_data(self):\n",
    "        assert (self.data_path/'imgs').is_dir(), \"Images folder not found\"\n",
    "        assert (self.data_path/'masks').is_dir(), \"Masks folder not found\"\n",
    "        assert (self.data_path/'files_trainable').exists(), \"Files trainable file not found\"\n",
    "        \n",
    "        print('Data Tree is found and ready for setup')\n",
    "        \n",
    "    \n",
    "    def setup(self, stage):\n",
    "        images_path = np.loadtxt(self.data_path/'files_trainable', dtype='str').tolist()\n",
    "        random.shuffle(images_path)\n",
    "        \n",
    "        '''\n",
    "        For now, we are validating on images ending with \"9.png\" and are seeing a categorical cross entropy loss of 0.051.\n",
    "        '''\n",
    "        \n",
    "        self.train_dataset = CommaLoader(data_path=self.data_path,\n",
    "                                         images_path=[x.split('masks/')[-1] for x in images_path if not x.endswith('9.png')],\n",
    "                                         preprocess_fn=self.preprocess_fn,\n",
    "                                         transforms=get_train_transforms(self.height, self.width),\n",
    "                                         class_values=self.class_values\n",
    "                                        )\n",
    "        \n",
    "        self.valid_dataset = CommaLoader(data_path=self.data_path,\n",
    "                                         images_path=[x.split('masks/')[-1] for x in images_path if x.endswith('9.png')],\n",
    "                                         preprocess_fn=self.preprocess_fn,\n",
    "                                         transforms=get_valid_transforms(self.height, self.width),\n",
    "                                         class_values=self.class_values\n",
    "                                        )\n",
    "        \n",
    "    def __dataloader(self, train):\n",
    "        # Train / Validation loaders\n",
    "        _dataset = self.train_dataset if train else self.valid_dataset\n",
    "        loader = DataLoader(dataset =_dataset,\n",
    "                            batch_size = self.batch_size,\n",
    "                            # num_workers = self.num_workers,\n",
    "                            shuffle=True if train else False)\n",
    "        return loader\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        log.info('Training data loaded.')\n",
    "        return self.__dataloader(train=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        log.info('Validation data loaded.')\n",
    "        return self.__dataloader(train=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a54a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832c4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up loggers for Callbacks\n",
    "log_path = Path('/Users/home/fun/comma10k-exp/')\n",
    "name = 'efficientnet-b0'\n",
    "version = 'first-stage'\n",
    "tb_logger = TensorBoardLogger(log_path, name=name, version=version)\n",
    "lr_logger = LearningRateLogger(logging_interval='epoch')\n",
    "ckpt_callback = ModelCheckpoint(filepath=Path(tb_logger.log_dir)/'checkpoints/{epoch:02d}_{val_loss:.4f}', \n",
    "                                              save_top_k=10, save_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7629871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45633b96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | net  | Unet | 6 M   \n",
      "Validation data loaded.\n",
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data loaded.\n",
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation data loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b90569913f4e9e95455dd1970a8056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(checkpoint_callback=ckpt_callback,\n",
    "                     logger=tb_logger,\n",
    "                     callbacks=[lr_logger],\n",
    "                    )\n",
    "\n",
    "trainer.logger.log_hyperparams(model.hparams)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e375b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
